[1710955761] 
llama server listening at http://127.0.0.1:8080

[1710955761] warming up the model with an empty run
[1710955761] Available slots:
[1710955761]  -> Slot 0 - max context: 2048
[1710955761] all slots are idle and system prompt is empty, clear the KV cache
[1710959884] slot 0 is processing [task id: 0]
[1710959884] slot 0 : kv cache rm - [0, end)
[1710959891] 
[1710959891] print_timings: prompt eval time =    5924.54 ms /  1035 tokens (    5.72 ms per token,   174.70 tokens per second)
[1710959891] print_timings:        eval time =     986.85 ms /    43 runs   (   22.95 ms per token,    43.57 tokens per second)
[1710959891] print_timings:       total time =    6911.39 ms
[1710959891] slot 0 released (1078 tokens in cache)
[1710960078] slot 0 is processing [task id: 45]
[1710960078] slot 0 : kv cache rm - [0, end)
[1710960084] 
[1710960084] print_timings: prompt eval time =    4982.39 ms /  1039 tokens (    4.80 ms per token,   208.53 tokens per second)
[1710960084] print_timings:        eval time =     869.43 ms /    38 runs   (   22.88 ms per token,    43.71 tokens per second)
[1710960084] print_timings:       total time =    5851.82 ms
[1710960084] slot 0 released (1077 tokens in cache)
[1710961328] slot 0 is processing [task id: 85]
[1710961328] slot 0 : kv cache rm - [0, end)
[1710961334] 
[1710961334] print_timings: prompt eval time =    4910.62 ms /  1027 tokens (    4.78 ms per token,   209.14 tokens per second)
[1710961334] print_timings:        eval time =    1006.46 ms /    44 runs   (   22.87 ms per token,    43.72 tokens per second)
[1710961334] print_timings:       total time =    5917.07 ms
[1710961334] slot 0 released (1071 tokens in cache)
